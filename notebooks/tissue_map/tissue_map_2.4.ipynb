{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2.3\n",
    "- Removed the optional terms from the matcher\n",
    "- Set all information taken from the XML file to lowercase\n",
    "- Added option to check for attribute name if there are no matches\n",
    "    - Will get fewer false positives from optional terms\n",
    "    - Set of attribute names reduced to attributes with confidence\n",
    "    - New check for attribute names uses a weaker matcher that allow optional terms\n",
    "\n",
    "\n",
    "## Issues\n",
    "- Can exclude proper nouns when looking for tissue terms to give fewer false positives\n",
    "    - BTO matches should only be nouns or adjectives\n",
    "- 'INSDC center name' gives some false positives\n",
    "- Paragraph can have 'too much' information\n",
    "- Cam look at attribute names vs title vs paragraph for ranking relevance of terms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready2 import *\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import xml.sax\n",
    "import csv\n",
    "import importlib\n",
    "import tissue_eval\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 6569\n"
     ]
    }
   ],
   "source": [
    "# Load the BTO ontology\n",
    "onto_path.append('../../data/ontologies/')\n",
    "onto = get_ontology('http://purl.obolibrary.org/obo/bto.owl').load()\n",
    "\n",
    "# classes dictionary: {class_name: class_label} \n",
    "#   - class_label is None if no label is found\n",
    "# class_synonyms dictionary: {class_name: [synonym1, synonym2, ...]}\n",
    "#   - synonym list is empty if no synonym is found\n",
    "classes = {c.name: c.label.first() for c in onto.classes()}\n",
    "class_synonyms = {c.name: c.hasExactSynonym + c.hasRelatedSynonym for c in onto.classes()}\n",
    "\n",
    "# added possible missing synonyms\n",
    "class_synonyms['BTO_0000440'] = class_synonyms['BTO_0000440'] + ['stool']\n",
    "\n",
    "# create a reverse mapping of classes and synonyms to BTO IDs\n",
    "classes_reverse = {c.label.first().lower(): c.name for c in onto.classes() if c.label != []}\n",
    "class_synonyms_reverse = {s.lower(): c for c, syn in class_synonyms.items() for s in syn}\n",
    "labels_reverse = {**classes_reverse, **class_synonyms_reverse}\n",
    "\n",
    "assert len(classes) == len(class_synonyms)\n",
    "print('Number of classes:', len(classes))\n",
    "\n",
    "# flatten the synonyms and class labels into a single set\n",
    "class_labels = {c for c in classes.values() if c is not None}\n",
    "class_synonyms_flattend = {s for syn in class_synonyms.values() for s in syn}\n",
    "bto_values = class_labels.union(class_synonyms_flattend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a spacy matcher to match patterns of BRENDA terms and synonyms\n",
    "#  - patterns are created by tokenizing the BRENDA terms and synonyms\n",
    "#  - matcher looks at only direct matches\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "patterns = []\n",
    "\n",
    "for bto_value in bto_values:\n",
    "    weak_pattern = [{'LOWER': token.lower_} for token in tokenizer(bto_value)]\n",
    "    patterns.append(weak_pattern)\n",
    "\n",
    "# takes the longest match if there are clashes\n",
    "matcher.add('bto', patterns, greedy='LONGEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attribute_matcher is a weaker matcher that allows optional terms at the end of the match\n",
    "redundant_end_terms = ['tissue', 'tissues', 'cell', 'cells']\n",
    "attribute_matcher = Matcher(nlp.vocab)\n",
    "\n",
    "\n",
    "weak_patterns = []\n",
    "for bto_value in bto_values:\n",
    "    weak_pattern = [{'LOWER': token.lower_} for token in tokenizer(bto_value)]\n",
    "    if weak_pattern[-1]['LOWER'] in redundant_end_terms:\n",
    "        weak_pattern[-1]['OP'] = '?'\n",
    "        weak_patterns.append(weak_pattern)\n",
    "\n",
    "\n",
    "attribute_matcher.add('bto', weak_patterns, greedy='LONGEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioSamplesMatcherHandler(xml.sax.ContentHandler):\n",
    "    '''\n",
    "    SAX handler class to read in information from a BioSamples XML file\n",
    "        - Reads the title, paragraph, and attributes of each BioSample\n",
    "        - Information stored in the provided sample_dict with the biosample_id as the key\n",
    "    '''\n",
    "    def __init__(self, sample_dict) -> None:\n",
    "        super().__init__()\n",
    "        self.sample_dict = sample_dict\n",
    "        self.attribute_dict = {}\n",
    "        self.biosample_id = ''\n",
    "        self.content_dict = {}\n",
    "        self.is_title = False\n",
    "        self.is_paragraph = False\n",
    "        self.attribute_name = ''\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        if name == 'BioSample':\n",
    "            self.biosample_id = attrs['accession']\n",
    "        elif name == 'Title':\n",
    "            self.is_title = True\n",
    "        elif name == 'Paragraph':\n",
    "            self.is_paragraph = True\n",
    "        elif name == 'Attribute':\n",
    "            try:\n",
    "                self.attribute_name = attrs['harmonized_name']\n",
    "            except KeyError:\n",
    "                self.attribute_name = attrs['attribute_name']\n",
    "\n",
    "    def characters(self, content):\n",
    "        if self.is_title:\n",
    "            self.content_dict['title'] = content.lower()\n",
    "            self.is_title = False\n",
    "        elif self.is_paragraph:\n",
    "            self.content_dict['paragraph'] = content.lower()\n",
    "            self.is_paragraph = False\n",
    "        elif self.attribute_name != '':\n",
    "            self.attribute_dict[self.attribute_name] = content.lower()\n",
    "            self.attribute_name = ''\n",
    "        \n",
    "\n",
    "    def endElement(self, name):\n",
    "        if name == 'BioSample':\n",
    "            self.content_dict['attributes'] = self.attribute_dict\n",
    "            self.sample_dict[self.biosample_id] = self.content_dict\n",
    "            self.attribute_dict = {}\n",
    "            self.content_dict = {}\n",
    "    \n",
    "    def endDocument(self):\n",
    "        print('Finished parsing BioSamples XML file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished parsing BioSamples XML file\n",
      "Number of samples: 10000\n"
     ]
    }
   ],
   "source": [
    "sample_dict = {}\n",
    "biosamples_path = '../../data/biosamples/biosample_random_samples.xml'\n",
    "\n",
    "parser = xml.sax.make_parser()\n",
    "handler = BioSamplesMatcherHandler(sample_dict)\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "parser.parse(biosamples_path)\n",
    "print('Number of samples:', len(sample_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates jsonl files for training in prodigy\n",
    "# - patterns.jsonl contains the patterns for the matcher\n",
    "# - biosamples_random_content.jsonl contains text from the title, paragraph, and attributes of each BioSample\n",
    "import json\n",
    "\n",
    "numbers_pattern = r'^[\\d\\. ]*$'\n",
    "punct_pattern = r\"\\ *[_&<>:-]+\\ *\"\n",
    "\n",
    "def not_tissue_term(content_str):\n",
    "    not_tissue_terms = {'not provided', 'not applicable', 'not collected', 'not available', 'none', 'undetected', 'unknown', 'none detected', 'na', 'missing', 'no', 'yes', 'n/a', 'true', 'false', 'dna', 'male', 'female', 'animal', 'public', 'human'}\n",
    "    if re.match(numbers_pattern, content_str):\n",
    "        return True\n",
    "    if content_str in not_tissue_terms:\n",
    "        return True\n",
    "    if len(content_str) < 3:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "redundant_end_terms = {'tissue', 'tissues', 'cell', 'cells'}\n",
    "with open('patterns.jsonl', 'w') as f:\n",
    "    for bto_term in bto_values:\n",
    "        pattern = [{'LOWER': token.lower_} for token in tokenizer(bto_term)]\n",
    "        if pattern[-1]['LOWER'] in redundant_end_terms:\n",
    "            pattern[-1]['OP'] = '?'\n",
    "        pattern_dict = {'label': 'BTO', 'pattern': pattern}\n",
    "        f.write(json.dumps(pattern_dict) + '\\n')\n",
    "\n",
    "\n",
    "\n",
    "with open('biosamples_random_content.jsonl', 'w') as f:\n",
    "    for biosample_id, content in sample_dict.items():\n",
    "        if 'title' in content:\n",
    "            term = re.sub(punct_pattern, \" \", content['title'])\n",
    "            if not_tissue_term(term):\n",
    "                continue\n",
    "            content_dict = {'text': term, 'meta': {'source': 'title', 'biosample_id': biosample_id}}\n",
    "            f.write(json.dumps(content_dict) + '\\n')\n",
    "        if 'paragraph' in content:\n",
    "            term = re.sub(punct_pattern, \" \", content['paragraph'])\n",
    "            if not_tissue_term(term):\n",
    "                continue\n",
    "            content_dict = {'text': term, 'meta': {'source': 'paragraph', 'biosample_id': biosample_id}}\n",
    "            f.write(json.dumps(content_dict) + '\\n')\n",
    "        if 'attributes' in content:\n",
    "            for attribute in content['attributes']:\n",
    "                term = re.sub(punct_pattern, \" \", content['attributes'][attribute])\n",
    "                if not_tissue_term(term):\n",
    "                    continue\n",
    "                content_dict = {'text': term, 'meta': {'source': 'attribute:' + attribute, 'biosample_id': biosample_id}}\n",
    "                f.write(json.dumps(content_dict) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds a regex pattern to the default tokenizer to split on underscores\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "tokenizer = nlp.tokenizer\n",
    "\n",
    "infixes = nlp.Defaults.infixes + [r'[_~]']\n",
    "infix_re = spacy.util.compile_infix_regex(infixes)\n",
    "tokenizer.infix_finditer = infix_re.finditer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_dict = {}\n",
    "confident_attributes = {'tissue', 'cell_type', 'cell_line', 'cell_subtype', 'source_name'}\n",
    "\n",
    "# finds all returned matches from the matcher\n",
    "# - matcher looks at the title, paragraph, and attributes\n",
    "# - returns a dictionary of matches for each sample\n",
    "\n",
    "for biosample_id, content_dict in sample_dict.items():\n",
    "    cur_matches = {}\n",
    "    title = content_dict['title']\n",
    "    attributes = content_dict['attributes'] # dictionary of attributes\n",
    "\n",
    "    title_tokens = tokenizer(title)\n",
    "    attributes_tokens = {key: tokenizer(value) for key, value in attributes.items()}\n",
    "\n",
    "    title_matches = matcher(title_tokens, as_spans=True)\n",
    "    attribute_matches = {}\n",
    "    for key, value in attributes_tokens.items():\n",
    "        attribute_match = matcher(value, as_spans=True)\n",
    "        if len(attribute_match) > 0:\n",
    "            attribute_matches[key] = matcher(value, as_spans=True)\n",
    "\n",
    "\n",
    "    if len(title_matches) > 0:\n",
    "        cur_matches['title'] = title_matches\n",
    "    if len(attribute_matches) > 0:\n",
    "        cur_matches['attributes'] = attribute_matches\n",
    "\n",
    "    if 'paragraph' in content_dict:\n",
    "        paragraph = content_dict['paragraph']\n",
    "        paragraph_tokens = tokenizer(paragraph)\n",
    "        paragraph_matches = matcher(paragraph_tokens, as_spans=True)\n",
    "        \n",
    "        if len(paragraph_matches) > 0:\n",
    "            cur_matches['paragraph'] = paragraph_matches\n",
    "\n",
    "    if cur_matches == {}:\n",
    "        for key, value in attributes_tokens.items():\n",
    "            if key not in confident_attributes:\n",
    "                continue\n",
    "            attribute_match = attribute_matcher(value, as_spans=True)\n",
    "            if len(attribute_match) > 0:\n",
    "                cur_matches[key] = attribute_match\n",
    "\n",
    "        if len(attribute_matches) > 0:\n",
    "            cur_matches['attributes'] = attribute_matches\n",
    "            \n",
    "    matches_dict[biosample_id] = cur_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive results: 5309\n",
      "Number of negative results: 4691\n"
     ]
    }
   ],
   "source": [
    "positive_samples = {key: value for key, value in matches_dict.items() if len(value) > 0}\n",
    "negative_samples = {key: value for key, value in matches_dict.items() if len(value) == 0}\n",
    "print('Number of positive results:', len(positive_samples))\n",
    "print('Number of negative results:', len(negative_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing results to CSV file\n"
     ]
    }
   ],
   "source": [
    "# take the whole attribute stuff to check results\n",
    "\n",
    "\n",
    "with open('../../data/biosamples/results/biosample_tissue_locations_2.3.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['biosample_accession_id', 'biosample_url', 'title_match', 'paragraph_match', 'attribute_matches'])\n",
    "    for biosample_id, matches in matches_dict.items():\n",
    "        title_match = ''\n",
    "        paragraph_match = ''\n",
    "        attribute_matches = ''\n",
    "\n",
    "        for match_type, match in matches.items():\n",
    "            if match_type == 'title':\n",
    "                for token in match:\n",
    "                    title_match += token.text + ' '\n",
    "            elif match_type == 'paragraph':\n",
    "                for token in match:\n",
    "                    paragraph_match += token.text + ' '\n",
    "            elif match_type == 'attributes':\n",
    "                for attribute, match in match.items():\n",
    "                    attribute_matches += f'{attribute},'\n",
    "                    for token in match:\n",
    "                        attribute_matches += token.text + ' '\n",
    "        \n",
    "        biosample_url = f'https://www.ncbi.nlm.nih.gov/biosample/{biosample_id}'\n",
    "        writer.writerow([biosample_id, biosample_url, title_match, paragraph_match, attribute_matches])\n",
    "\n",
    "\n",
    "print('Finished writing results to CSV file')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
