{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n",
      "\n",
      "Warning: SQLite3 version 3.40.0 and 3.41.2 have huge performance regressions; please install version 3.41.1 or 3.42!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from owlready2 import *\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import xml.sax\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Pass 1:\n",
    "- finds matches from a set of confident attributes using a Spacy matcher\n",
    "    - preprocessing performed to remove punctuation adn convert to lowercase\n",
    "- samples with no matches have attributes, title, and paragraph written to the the tissue_tmp_1.jsonl file\n",
    "\n",
    "After first pass: 10519273 samples matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_str):\n",
    "    punct_pattern = r'\\ *[_&<>:-]+\\ *'\n",
    "    input_str = re.sub(punct_pattern, ' ', input_str)\n",
    "    return input_str.lower().strip()\n",
    "\n",
    "# load the BRENDA ontology\n",
    "onto = get_ontology('http://purl.obolibrary.org/obo/bto.owl').load()\n",
    "\n",
    "# load classes and synonyms\n",
    "classes = {c.name: c.label.first() for c in onto.classes()}\n",
    "class_synonyms = {c.name: c.hasExactSynonym + c.hasRelatedSynonym for c in onto.classes()}\n",
    "\n",
    "# add additional synonyms\n",
    "class_synonyms['BTO_0000440'] += ['stool']\n",
    "class_synonyms['BTO_0005238'] += ['HEK293-T-REx']\n",
    "\n",
    "# create a reverse mapping of labels to BRENDA ids\n",
    "classes_reverse = {preprocess(c.label.first()): c.name for c in onto.classes() if c.label != []}\n",
    "class_synonyms_reverse = {preprocess(s): c for c, syn in class_synonyms.items() for s in syn}\n",
    "labels_reverse = {**classes_reverse, **class_synonyms_reverse}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse mapping removes redundant end terms from the key where possible\n",
    "redundant_end_terms = ('tissue', 'tissues', 'cell', 'cells')\n",
    "additional_labels = {}\n",
    "for key, value in labels_reverse.items():\n",
    "    if key.endswith(redundant_end_terms):\n",
    "        # add the key without the redundant end term\n",
    "        additional_labels[' '.join(key.split()[:-1])] = value\n",
    "\n",
    "for key, value in additional_labels.items():\n",
    "    if key not in labels_reverse:\n",
    "        labels_reverse[key] = value\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "# blank matcher with vocab from the large model\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# standard tokenizer from spacy\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "patterns = []\n",
    "for label in labels_reverse.keys():\n",
    "    if label != '':\n",
    "        tokens = tokenizer(label)\n",
    "        pattern = [{'LOWER': token.lower_} for token in tokens]\n",
    "        patterns.append(pattern)\n",
    "\n",
    "matcher.add('bto', patterns, greedy='LONGEST')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioSamplesHandler(xml.sax.ContentHandler):\n",
    "    '''\n",
    "    SAX hander class to read in in formation from a BioSamples XMl file\n",
    "    - Reads in the title, paragraph, and attributes of each sample into a dictionary\n",
    "        - if no matching attribtue found, dictionary written as json to file for second pass\n",
    "    - Found values written to provided sample_dict with the biosample_id as the key\n",
    "    '''\n",
    "\n",
    "    def __init__(self, positive_attributes, tmp_file, output_file):\n",
    "        self.positive_attributes = positive_attributes\n",
    "        self.tmp_file = tmp_file\n",
    "        self.output_file = output_file\n",
    "        self.biosample_id = ''\n",
    "        self.cur_dict = {}\n",
    "        self.attributes = {}\n",
    "        self.attribute_name = ''\n",
    "        self.is_title = False\n",
    "        self.is_paragraph = False\n",
    "        self.is_sra = False\n",
    "        self.sra_id = ''\n",
    "        self.count = 0\n",
    "    \n",
    "    def startDocument(self):\n",
    "        open(self.tmp_file, 'w').close()\n",
    "        open(self.output_file, 'w').close()\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        if name == 'BioSample':\n",
    "            self.biosample_id = attrs['accession']\n",
    "        elif name == 'Title':\n",
    "            self.is_title = True\n",
    "        elif name == 'Paragraph':\n",
    "            self.is_paragraph = True\n",
    "        elif name == 'Attribute':\n",
    "            try:\n",
    "                self.attribute_name = attrs['harmonized_name']\n",
    "            except KeyError:\n",
    "                self.attribute_name = attrs['attribute_name']\n",
    "        elif name == 'Id':\n",
    "            if 'db' in attrs and attrs['db'] == 'SRA':\n",
    "                self.is_sra = True\n",
    "\n",
    "    def characters(self, content):\n",
    "        if self.is_title:\n",
    "            self.cur_dict['title'] = content.lower()\n",
    "            self.is_title = False\n",
    "        elif self.is_paragraph:\n",
    "            self.cur_dict['paragraph'] = content.lower()\n",
    "            self.is_paragraph = False\n",
    "        elif self.is_sra:\n",
    "            self.sra_id = content\n",
    "            self.is_sra = False\n",
    "        elif self.attribute_name != '':\n",
    "            self.attributes[self.attribute_name] = content.lower()\n",
    "            self.attribute_name = ''\n",
    "\n",
    "    def endElement(self, name):\n",
    "        if name == 'BioSample':\n",
    "            # check if there are any positive attributes and extract the tissue\n",
    "            intersect = set(self.attributes.keys()).intersection(self.positive_attributes)\n",
    "            matches = []\n",
    "            if len(intersect) == 1:\n",
    "                attribute_value = preprocess(self.attributes[list(intersect)[0]])\n",
    "                # check if the attribute value is a BTO term using the matcher\n",
    "                attribute_tokens = tokenizer(attribute_value)\n",
    "                matches = matcher(attribute_tokens, as_spans=True)\n",
    "            elif len(intersect) > 1:\n",
    "                # if there are multiple positive attributes, check if any of the values are BTO terms\n",
    "                for attribute in intersect:\n",
    "                    attribute_value = preprocess(self.attributes[attribute])\n",
    "                    attribute_tokens = tokenizer(attribute_value)\n",
    "                    matches += matcher(attribute_tokens, as_spans=True)\n",
    "\n",
    "            if matches == []:\n",
    "                # no matches found, write the biosample content to a tmp file for second pass\n",
    "                with open(self.tmp_file, 'a') as f:\n",
    "                    self.cur_dict['biosample_id'] = self.biosample_id\n",
    "                    self.cur_dict['attributes'] = self.attributes\n",
    "                    self.cur_dict['sra_id'] = self.sra_id\n",
    "                    json.dump(self.cur_dict, f)\n",
    "                    f.write('\\n')\n",
    "            else:\n",
    "                # matches found, add to output file\n",
    "                tissue_matches = ','.join({m.text for m in matches})\n",
    "                bto_matches = ','.join({labels_reverse[m.text.lower()] for m in matches})\n",
    "                with open(self.output_file, 'a') as f:\n",
    "                    json.dump({'biosample_id': self.biosample_id , 'sra_id': self.sra_id, 'tissue': tissue_matches, 'bto_matches': bto_matches}, f)\n",
    "                    f.write('\\n')\n",
    "                \n",
    "            self.attribute_name = ''\n",
    "            self.attributes = {}\n",
    "            self.cur_dict = {}\n",
    "            self.biosample_id = ''\n",
    "            self.sra_id = ''\n",
    "            self.count += 1\n",
    "            if self.count % 1000000 == 0:\n",
    "                print(self.count, \"samples parsed\")\n",
    "\n",
    "\n",
    "    def endDocument(self):\n",
    "        print(\"Finished parsing BioSamples XML file\")\n",
    "                \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some BioSample entries have no SRA identifier listed but SRA data is available from the link:\n",
    "https://www.ncbi.nlm.nih.gov/sra?LinkName=biosample_sra&from_uid=BIOSAMPLE_NUMERICAL\n",
    "\n",
    "where BIOSAMPLE_ID is the numerical ID of the biosample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 samples parsed\n",
      "2000000 samples parsed\n",
      "3000000 samples parsed\n",
      "4000000 samples parsed\n",
      "5000000 samples parsed\n",
      "6000000 samples parsed\n",
      "7000000 samples parsed\n",
      "8000000 samples parsed\n",
      "9000000 samples parsed\n",
      "10000000 samples parsed\n",
      "11000000 samples parsed\n",
      "12000000 samples parsed\n",
      "13000000 samples parsed\n",
      "14000000 samples parsed\n",
      "15000000 samples parsed\n",
      "16000000 samples parsed\n",
      "17000000 samples parsed\n",
      "18000000 samples parsed\n",
      "19000000 samples parsed\n",
      "20000000 samples parsed\n",
      "21000000 samples parsed\n",
      "22000000 samples parsed\n",
      "23000000 samples parsed\n",
      "24000000 samples parsed\n",
      "25000000 samples parsed\n",
      "26000000 samples parsed\n",
      "27000000 samples parsed\n",
      "28000000 samples parsed\n",
      "29000000 samples parsed\n",
      "30000000 samples parsed\n",
      "31000000 samples parsed\n",
      "32000000 samples parsed\n",
      "33000000 samples parsed\n",
      "Finished parsing BioSamples XML file\n"
     ]
    }
   ],
   "source": [
    "positive_attributes = {'tissue', 'cell_type', 'cell_line', 'cell_subtype', 'source_name', 'host_tissue_sampled', 'body_habitat', 'body_product', 'host_body_product', 'host_body_habitat'}\n",
    "\n",
    "# parse the BioSamples XML file\n",
    "parser = xml.sax.make_parser()\n",
    "handler = BioSamplesHandler(positive_attributes, '/home/ec2-user/workspace/data/results/tissue_tmp_1.jsonl', '/home/ec2-user/workspace/data/results/tissue_output.jsonl')\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "# comment out the following line to rerun the parser\n",
    "# parser.parse('/home/ec2-user/workspace/data/biosample_set.xml')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Pass 2\n",
    "- Runs the Spacy matcher with no optional terms on all the attributes\n",
    "\n",
    "!! Code was run in a sperate file using GNU parallel for faster execution !!\n",
    "\n",
    "After second pass: \n",
    "- Additional 6205050 samples matched    \n",
    "- Total 16724323 samples matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready2 import *\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import xml.sax\n",
    "import orjson as json\n",
    "import re\n",
    "import sys\n",
    "\n",
    "def preprocess(input_str):\n",
    "    punct_pattern = r'\\ *[_&<>:-]+\\ *'\n",
    "    input_str = re.sub(punct_pattern, ' ', input_str)   \n",
    "    return input_str.lower().strip()\n",
    "\n",
    "\n",
    "# file will read an input file that is in the format tissue_tmp_1ax and will output a file in the format tissue_tmp_2ax.jsonl\n",
    "\n",
    "# load the BRENDA ontology\n",
    "onto = get_ontology('http://purl.obolibrary.org/obo/bto.owl').load()\n",
    "\n",
    "# load classes and synonyms\n",
    "classes = {c.name: c.label.first() for c in onto.classes()}\n",
    "class_synonyms = {c.name: c.hasExactSynonym + c.hasRelatedSynonym for c in onto.classes()}\n",
    "\n",
    "# add additional synonyms\n",
    "class_synonyms['BTO_0000440'] += ['stool']\n",
    "class_synonyms['BTO_0005238'] += ['HEK293-T-REx']\n",
    "\n",
    "# create a reverse mapping of labels to BRENDA ids\n",
    "classes_reverse = {preprocess(c.label.first()): c.name for c in onto.classes() if c.label != []}\n",
    "class_synonyms_reverse = {preprocess(s): c for c, syn in class_synonyms.items() for s in syn}\n",
    "labels_reverse = {**classes_reverse, **class_synonyms_reverse}\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "# blank matcher with vocab from the large model\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# standard tokenizer from spacy\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "patterns = []\n",
    "for label in labels_reverse.keys():\n",
    "    tokens = tokenizer(label)\n",
    "    pattern = [{'LOWER': token.lower_} for token in tokens]\n",
    "    patterns.append(pattern)\n",
    "\n",
    "matcher.add('bto', patterns, greedy='LONGEST')\n",
    "\n",
    "\n",
    "assert len(sys.argv) == 2, \"Please provide an input file\"\n",
    "input_file = sys.argv[1]\n",
    "output_file = input_file.replace('tmp_1', 'output_2')\n",
    "tmp_file = input_file.replace('tmp_1', 'tmp_2')\n",
    "\n",
    "open(output_file, 'w').close()\n",
    "open(tmp_file, 'w').close()\n",
    "\n",
    "with open(input_file, 'r') as f:\n",
    "    print(\"Starting second pass for \", input_file)\n",
    "    for line in f:\n",
    "        cur_dict = {} # stores the data to be written to the output file if applicable\n",
    "        matches = [] # stores the matches for the current biosample\n",
    "        # biosample_dict stores the biosample content\n",
    "        biosample_dict = json.loads(line)\n",
    "        cur_dict['biosample_id'] = biosample_dict['biosample_id']\n",
    "        cur_dict['sra_id'] = biosample_dict['sra_id']\n",
    "        # attributes is a dictionary of the biosample attributes\n",
    "        attributes = biosample_dict['attributes']\n",
    "        for attribute_name in attributes:\n",
    "            attribute_value = preprocess(attributes[attribute_name])\n",
    "            attribute_tokens = tokenizer(attribute_value)\n",
    "            matches += matcher(attribute_tokens, as_spans=True)\n",
    "\n",
    "        if matches != []:\n",
    "            tissue_matches = ','.join({m.text for m in matches})\n",
    "            bto_matches = ','.join({labels_reverse[m.text.lower()] for m in matches})\n",
    "            cur_dict['tissue'] = tissue_matches\n",
    "            cur_dict['bto_matches'] = bto_matches\n",
    "            with open(output_file, 'ab') as f:\n",
    "                f.write(json.dumps(cur_dict))\n",
    "                f.write(b'\\n')\n",
    "\n",
    "        else:\n",
    "            # no matches found in the attributes, writes the biosample content to a tmp file for third pass\n",
    "            with open(tmp_file, 'ab') as f:\n",
    "                f.write(json.dumps(biosample_dict))\n",
    "                f.write(b'\\n')\n",
    "\n",
    "    print(\"Finished parsing\", input_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Pass 1 Re-run:\n",
    "- finds matches from a set of confident attributes using a Spacy matcher\n",
    "    - preprocessing performed to remove punctuation adn convert to lowercase\n",
    "- samples with no matches have attributes, title, and paragraph written to the the tissue_tmp_1.jsonl file\n",
    "## Changes\n",
    "- want to re run pass 1 to update the tissue_output_1.jsonl with more info\n",
    "    - store the exact value from the biosamples metadata that the match came from\n",
    "    - store the location that the match came from (attribute X, title, paragraph...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready2 import *\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import xml.sax\n",
    "import json\n",
    "import re\n",
    "\n",
    "def preprocess(input_str):\n",
    "    punct_pattern = r'\\ *[_&<>:-]+\\ *'\n",
    "    input_str = re.sub(punct_pattern, ' ', input_str)\n",
    "    return input_str.lower().strip()\n",
    "\n",
    "# load the BRENDA ontology\n",
    "onto = get_ontology('http://purl.obolibrary.org/obo/bto.owl').load()\n",
    "\n",
    "# load classes and synonyms\n",
    "classes = {c.name: c.label.first() for c in onto.classes()}\n",
    "class_synonyms = {c.name: c.hasExactSynonym + c.hasRelatedSynonym for c in onto.classes()}\n",
    "\n",
    "# add additional synonyms\n",
    "class_synonyms['BTO_0000440'] += ['stool']\n",
    "class_synonyms['BTO_0005238'] += ['HEK293-T-REx']\n",
    "\n",
    "# create a reverse mapping of labels to BRENDA ids\n",
    "classes_reverse = {preprocess(c.label.first()): c.name for c in onto.classes() if c.label != []}\n",
    "class_synonyms_reverse = {preprocess(s): c for c, syn in class_synonyms.items() for s in syn}\n",
    "labels_reverse = {**classes_reverse, **class_synonyms_reverse}\n",
    "\n",
    "# reverse mapping removes redundant end terms from the key where possible\n",
    "redundant_end_terms = ('tissue', 'tissues', 'cell', 'cells')\n",
    "additional_labels = {}\n",
    "for key, value in labels_reverse.items():\n",
    "    if key.endswith(redundant_end_terms):\n",
    "        # add the key without the redundant end term\n",
    "        additional_labels[' '.join(key.split()[:-1])] = value\n",
    "\n",
    "for key, value in additional_labels.items():\n",
    "    if key not in labels_reverse:\n",
    "        labels_reverse[key] = value\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "# blank matcher with vocab from the large model\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# standard tokenizer from spacy\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "patterns = []\n",
    "for label in labels_reverse.keys():\n",
    "    if label != '':\n",
    "        tokens = tokenizer(label)\n",
    "        pattern = [{'LOWER': token.lower_} for token in tokens]\n",
    "        patterns.append(pattern)\n",
    "\n",
    "matcher.add('bto', patterns, greedy='LONGEST')\n",
    "\n",
    "class BioSamplesHandler(xml.sax.ContentHandler):\n",
    "    '''\n",
    "    SAX hander class to read in in formation from a BioSamples XMl file\n",
    "    - Reads in the title, paragraph, and attributes of each sample into a dictionary\n",
    "        - if no matching attribtue found, dictionary written as json to file for second pass\n",
    "    - Found values written to provided sample_dict with the biosample_id as the key\n",
    "    '''\n",
    "\n",
    "    def __init__(self, positive_attributes, tmp_file, output_file):\n",
    "        self.positive_attributes = positive_attributes\n",
    "        self.output_file = output_file\n",
    "        self.tmp_file = tmp_file\n",
    "        self.biosample_id = ''\n",
    "        self.cur_dict = {}\n",
    "        self.attributes = {}\n",
    "        self.attribute_name = ''\n",
    "        self.is_title = False\n",
    "        self.is_paragraph = False\n",
    "        self.is_sra = False\n",
    "        self.sra_id = ''\n",
    "        self.count = 0\n",
    "\n",
    "    \n",
    "    def startDocument(self):\n",
    "        open(self.output_file, 'w').close()\n",
    "        open(self.tmp_file, 'w').close()\n",
    "        print(\"Parsing BioSamples XML file...\")\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        if name == 'BioSample':\n",
    "            self.biosample_id = attrs['accession']\n",
    "        elif name == 'Title':\n",
    "            self.is_title = True\n",
    "        elif name == 'Paragraph':\n",
    "            self.is_paragraph = True\n",
    "        elif name == 'Attribute':\n",
    "            try:\n",
    "                self.attribute_name = attrs['harmonized_name']\n",
    "            except KeyError:\n",
    "                self.attribute_name = attrs['attribute_name']\n",
    "        elif name == 'Id':\n",
    "            if 'db' in attrs and attrs['db'] == 'SRA':\n",
    "                self.is_sra = True\n",
    "\n",
    "    def characters(self, content):\n",
    "        if self.is_title:\n",
    "            self.cur_dict['title'] = content.lower()\n",
    "            self.is_title = False\n",
    "        elif self.is_paragraph:\n",
    "            self.cur_dict['paragraph'] = content.lower()\n",
    "            self.is_paragraph = False\n",
    "        elif self.is_sra:\n",
    "            self.sra_id = content\n",
    "            self.is_sra = False\n",
    "        elif self.attribute_name != '':\n",
    "            self.attributes[self.attribute_name] = content.lower()\n",
    "            self.attribute_name = ''\n",
    "\n",
    "    def endElement(self, name):\n",
    "        if name == 'BioSample':\n",
    "            attribute_vals = []\n",
    "            text_matches = []\n",
    "            # check if there are any positive attributes and extract the tissue\n",
    "            intersect = set(self.attributes.keys()).intersection(self.positive_attributes)\n",
    "            matches = []\n",
    "            if len(intersect) == 1:\n",
    "                attribute_value = preprocess(self.attributes[list(intersect)[0]])\n",
    "                # check if the attribute value is a BTO term using the matcher\n",
    "                attribute_tokens = tokenizer(attribute_value)\n",
    "                matches = matcher(attribute_tokens, as_spans=True)\n",
    "                # add the attribute name and value to a list if there is a match\n",
    "                if matches != []:\n",
    "                    # add the attribute name and text for each match\n",
    "                    for _ in matches:\n",
    "                        attribute_vals.append(list(intersect)[0])\n",
    "                        text_matches.append(self.attributes[list(intersect)[0]])\n",
    "\n",
    "            elif len(intersect) > 1:\n",
    "                # if there are multiple positive attributes, check if any of the values are BTO terms\n",
    "                for attribute in intersect:\n",
    "                    attribute_value = preprocess(self.attributes[attribute])\n",
    "                    attribute_tokens = tokenizer(attribute_value) # check if the attribute value is a BTO term using the matcher\n",
    "                    tmp_matches = matcher(attribute_tokens, as_spans=True)\n",
    "                    matches += tmp_matches\n",
    "                    if tmp_matches != []:\n",
    "                        # add the attribute name and text for each match\n",
    "                        for _ in tmp_matches:\n",
    "                            attribute_vals.append(attribute)\n",
    "                            text_matches.append(self.attributes[attribute])\n",
    "\n",
    "            if matches == []:\n",
    "                # no matches found, write to tmp file\n",
    "                with open(self.tmp_file, 'a') as f:\n",
    "                    self.cur_dict['biosample_id'] = self.biosample_id\n",
    "                    self.cur_dict['attributes'] = self.attributes\n",
    "                    self.cur_dict['sra_id'] = self.sra_id\n",
    "                    json.dump(self.cur_dict, f)\n",
    "                    f.write('\\n')\n",
    "\n",
    "            else:\n",
    "                # matches found, add to output file\n",
    "                tissue_matches = ','.join([m.text for m in matches])\n",
    "                bto_matches = ','.join([labels_reverse[m.text.lower()] for m in matches])\n",
    "                attributes = ','.join(attribute_vals)\n",
    "                text = ','.join(text_matches)\n",
    "                with open(self.output_file, 'a') as f:\n",
    "                    json.dump({'biosample_id': self.biosample_id , 'sra_id': self.sra_id, 'source': attributes, 'text': text, 'tissue': tissue_matches, 'bto_matches': bto_matches}, f)\n",
    "                    f.write('\\n')\n",
    "                \n",
    "            self.attribute_name = ''\n",
    "            self.attributes = {}\n",
    "            self.cur_dict = {}\n",
    "            self.biosample_id = ''\n",
    "            self.sra_id = ''\n",
    "            self.count += 1\n",
    "            if self.count % 1000000 == 0:\n",
    "                print(self.count, \"samples parsed\")\n",
    "\n",
    "\n",
    "    def endDocument(self):\n",
    "        print(self.count, \"samples parsed\")\n",
    "        print(\"Finished parsing BioSamples XML file\")\n",
    "                \n",
    "\n",
    "positive_attributes = {'tissue', 'cell_type', 'cell_line', 'cell_subtype', 'source_name', 'host_tissue_sampled', 'body_habitat', 'body_product', 'host_body_product', 'host_body_habitat'}\n",
    "\n",
    "# parse the BioSamples XML file\n",
    "parser = xml.sax.make_parser()\n",
    "handler = BioSamplesHandler(positive_attributes, '/home/ec2-user/workspace/data/results/tissue_tmp_rerun_1.jsonl', '/home/ec2-user/workspace/data/results/tissue_output_1_rerun.jsonl')\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "# comment out the following line to rerun the parser\n",
    "parser.parse('/home/ec2-user/workspace/data/biosample_set.xml')\n",
    "\n",
    "# biosample_set.xml.gz redownloaded on 2023/09/15\n",
    "# code run on 2023/09/15 at 1435 in a separate python file\n",
    "\n",
    "# biosample_set.xml.gz redownloaded on 2023/10/06\n",
    "# code run on 2023/10/06 at 1840 in a separate python file\n",
    "# total samples parsed: 35369235\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Pass 2 Re-run\n",
    "- stores more information about where the exact text match came from \n",
    "- run in parallel in separate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready2 import *\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokenizer import Tokenizer\n",
    "# import orjson as json\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(input_str):\n",
    "    punct_pattern = r'\\ *[_&<>:-]+\\ *'\n",
    "    input_str = re.sub(punct_pattern, ' ', input_str)\n",
    "    return input_str.lower().strip()\n",
    "\n",
    "\n",
    "# file will read an input file that is in the format tissue_tmp_1ax and will output a file in the format tissue_tmp_2ax.jsonl\n",
    "\n",
    "# load the BRENDA ontology\n",
    "onto = get_ontology('http://purl.obolibrary.org/obo/bto.owl').load()\n",
    "\n",
    "# load classes and synonyms\n",
    "classes = {c.name: c.label.first() for c in onto.classes()}\n",
    "class_synonyms = {c.name: c.hasExactSynonym + c.hasRelatedSynonym for c in onto.classes()}\n",
    "\n",
    "# add additional synonyms\n",
    "class_synonyms['BTO_0000440'] += ['stool']\n",
    "class_synonyms['BTO_0005238'] += ['HEK293-T-REx']\n",
    "\n",
    "# create a reverse mapping of labels to BRENDA ids\n",
    "classes_reverse = {preprocess(c.label.first()): c.name for c in onto.classes() if c.label != []}\n",
    "class_synonyms_reverse = {preprocess(s): c for c, syn in class_synonyms.items() for s in syn}\n",
    "labels_reverse = {**classes_reverse, **class_synonyms_reverse}\n",
    "\n",
    "# DON'T REMOVE REDUNDANT END TERMS\n",
    "# - leads to false positives for terms like \"M cell\" and \"T cell\"\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "# blank matcher with vocab from the large model\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# standard tokenizer from spacy\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "patterns = []\n",
    "for label in labels_reverse.keys():\n",
    "    tokens = tokenizer(label)\n",
    "    pattern = [{'LOWER': token.lower_} for token in tokens]\n",
    "    patterns.append(pattern)\n",
    "\n",
    "matcher.add('bto', patterns, greedy='LONGEST')\n",
    "\n",
    "\n",
    "assert len(sys.argv) == 2, \"Please provide an input file\"\n",
    "input_file = sys.argv[1]\n",
    "output_file = input_file.replace('tmp_rerun_1', 'output_2_rerun')\n",
    "tmp_file = input_file.replace('tmp_rerun_1', 'tmp_2_rerun')\n",
    "\n",
    "open(output_file, 'w').close()\n",
    "open(tmp_file, 'w').close()\n",
    "\n",
    "with open(input_file, 'r') as f:\n",
    "    print(\"Starting second pass for \", input_file)\n",
    "    for line in f:\n",
    "        cur_dict = {} # stores the data to be written to the output file if applicable\n",
    "        matches = [] # stores the matches for the current biosample\n",
    "        matched_attributes = [] # stores the matched attributes for the current biosample\n",
    "        text_matches = [] # stores the text matches for the current biosample\n",
    "        # biosample_dict stores the biosample content\n",
    "        biosample_dict = json.loads(line)\n",
    "        cur_dict['biosample_id'] = biosample_dict['biosample_id']\n",
    "        cur_dict['sra_id'] = biosample_dict['sra_id']\n",
    "        # attributes is a dictionary of the biosample attributes\n",
    "        attributes = biosample_dict['attributes']\n",
    "        for attribute_name in attributes:\n",
    "            attribute_value = preprocess(attributes[attribute_name])\n",
    "            attribute_tokens = tokenizer(attribute_value)\n",
    "            cur_match = matcher(attribute_tokens, as_spans=True)\n",
    "            matches += cur_match\n",
    "            if cur_match != []:\n",
    "                for _ in cur_match:\n",
    "                    matched_attributes += [attribute_name]\n",
    "                    text_matches += [attributes[attribute_name]]\n",
    "\n",
    "\n",
    "        # if matches are found, writes the biosample content to the output file\n",
    "        if matches != []:\n",
    "            tissue_matches = ','.join({m.text for m in matches})\n",
    "            bto_matches = ','.join({labels_reverse[m.text.lower()] for m in matches})\n",
    "            attribute_matches = ','.join(matched_attributes)\n",
    "            text = ','.join(text_matches)\n",
    "            cur_dict['source'] = attribute_matches\n",
    "            cur_dict['text'] = text\n",
    "            cur_dict['tissue'] = tissue_matches\n",
    "            cur_dict['bto_matches'] = bto_matches\n",
    "            with open(output_file, 'a') as f:\n",
    "                json.dump(cur_dict, f)\n",
    "                f.write('\\n')\n",
    "\n",
    "        else:\n",
    "            # no matches found in the attributes, writes the biosample content to a tmp file for third pass\n",
    "            with open(tmp_file, 'a') as f:\n",
    "                json.dump(biosample_dict, f)\n",
    "                f.write('\\n')\n",
    "\n",
    "    print(\"Finished parsing\", input_file)\n",
    "\n",
    "# code run on 2023/10/08 at 1425 in a separate python file for parallelization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commands run for pass 2 file\n",
    "```\n",
    "split -1000000 tissue_tmp_rerun_1.jsonl tissue_tmp_rerun_1_\n",
    "ls tissue_tmp_rerun_1_a* | parallel python tissue_map_pass_2_parallel.py\n",
    "\n",
    "cat tissue_output_2_rerun_a* >> tissue_output_2_rerun.jsonl\n",
    "cat tissue_tmp_2_rerun_a* >> tissue_tmp_2_rerun.jsonl\n",
    "```\n",
    "- move file back to the results folder\n",
    "\n",
    "## Rerun results\n",
    "- first pass: 10954681\n",
    "- second pass: 6488557\n",
    "- 17443238 total samples labelled\n",
    "- 17652236 no labels\n",
    "\n",
    "\n",
    "## Rerun round 2 results\n",
    "- first pass: 11118049\n",
    "- second pass: 6557376\n",
    "- 17675425 total samples labelled\n",
    "- 17693810 no labels\n",
    "- 35369235 total samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISSUES with initial rerun\n",
    "- when attributes have more than one match in them, the attributes are only written once\n",
    "\n",
    "`{\"biosample_id\": \"SAMN05153636\", \"sra_id\": \"SRS1456259\", \"source\": \"cell_type,source_name\", \"text\": \"haematopoietic stem cell/multipotent progenitor,bone marrow\", \"tissue\": \"haematopoietic,stem,progenitor,bone marrow\", \"bto_matches\": \"BTO_0000574,BTO_0001300,BTO_0000725,BTO_0000141\"}`\n",
    "\n",
    "- leads to issues when importing into csv as number of matches per attribute (source) is not stored and only the numebr of sources are entered into the database\n",
    "\n",
    "## Fixes\n",
    "- add attribute and text matches in a loop for each match found\n",
    "- attribute and text matches fields will have the same number of comma separated values as the tissue_match fields"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
