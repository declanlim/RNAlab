{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2.3\n",
    "- Removed the optional terms from the matcher\n",
    "- Set all information taken from the XML file to lowercase\n",
    "- Added option to check for attribute name if there are no matches\n",
    "    - Will get fewer false positives from optional terms\n",
    "    - Set of attribute names reduced to attributes with confidence\n",
    "    - New check for attribute names uses a weaker matcher that allow optional terms\n",
    "\n",
    "\n",
    "## Issues\n",
    "- Can use POS tagging from spacy to reduce numebr of false positives\n",
    "    - BTO matches should only be nouns or adjectives\n",
    "    - Issues arise when 'normal' nouns are capitalised\n",
    "- 'INSDC center name' gives some false positives\n",
    "- Paragraph can have 'too much' information\n",
    "- Cam look at attribute names vs title vs paragraph for ranking relevance of terms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n",
      "\n",
      "Warning: SQLite3 version 3.40.0 and 3.41.2 have huge performance regressions; please install version 3.41.1 or 3.42!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from owlready2 import *\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import xml.sax\n",
    "import csv\n",
    "import importlib\n",
    "import tissue_eval\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 6569\n"
     ]
    }
   ],
   "source": [
    "# Load the BTO ontology\n",
    "onto_path.append('../../data/ontologies/')\n",
    "onto = get_ontology('http://purl.obolibrary.org/obo/bto.owl').load()\n",
    "\n",
    "# classes dictionary: {class_name: class_label} \n",
    "#   - class_label is None if no label is found\n",
    "# class_synonyms dictionary: {class_name: [synonym1, synonym2, ...]}\n",
    "#   - synonym list is empty if no synonym is found\n",
    "classes = {c.name: c.label.first() for c in onto.classes()}\n",
    "class_synonyms = {c.name: c.hasExactSynonym + c.hasRelatedSynonym for c in onto.classes()}\n",
    "\n",
    "# added possible missing synonyms\n",
    "class_synonyms['BTO_0000440'] = class_synonyms['BTO_0000440'] + ['stool']\n",
    "\n",
    "# create a reverse mapping of classes and synonyms to BTO IDs\n",
    "classes_reverse = {c.label.first().lower(): c.name for c in onto.classes() if c.label != []}\n",
    "class_synonyms_reverse = {s.lower(): c for c, syn in class_synonyms.items() for s in syn}\n",
    "labels_reverse = {**classes_reverse, **class_synonyms_reverse}\n",
    "\n",
    "assert len(classes) == len(class_synonyms)\n",
    "print('Number of classes:', len(classes))\n",
    "\n",
    "# flatten the synonyms and class labels into a single set\n",
    "class_labels = {c for c in classes.values() if c is not None}\n",
    "class_synonyms_flattend = {s for syn in class_synonyms.values() for s in syn}\n",
    "bto_values = class_labels.union(class_synonyms_flattend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a spacy matcher to match patterns of BRENDA terms and synonyms\n",
    "#  - patterns are created by tokenizing the BRENDA terms and synonyms\n",
    "#  - matcher looks at only direct matches\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "\n",
    "patterns = []\n",
    "\n",
    "for bto_value in bto_values:\n",
    "    weak_pattern = [{'LOWER': token.lower_} for token in tokenizer(bto_value)]\n",
    "    patterns.append(weak_pattern)\n",
    "\n",
    "# takes the longest match if there are clashes\n",
    "matcher.add('bto', patterns, greedy='LONGEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attribute_matcher is a weaker matcher that allows optional terms at the end of the match\n",
    "redundant_end_terms = ['tissue', 'tissues', 'cell', 'cells']\n",
    "attribute_matcher = Matcher(nlp.vocab)\n",
    "\n",
    "\n",
    "weak_patterns = []\n",
    "for bto_value in bto_values:\n",
    "    weak_pattern = [{'LOWER': token.lower_} for token in tokenizer(bto_value)]\n",
    "    if weak_pattern[-1]['LOWER'] in redundant_end_terms:\n",
    "        weak_pattern[-1]['OP'] = '?'\n",
    "        weak_patterns.append(weak_pattern)\n",
    "\n",
    "\n",
    "attribute_matcher.add('bto', weak_patterns, greedy='LONGEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioSamplesMatcherHandler(xml.sax.ContentHandler):\n",
    "    '''\n",
    "    SAX handler class to read in information from a BioSamples XML file\n",
    "        - Reads the title, paragraph, and attributes of each BioSample\n",
    "        - Information stored in the provided sample_dict with the biosample_id as the key\n",
    "    '''\n",
    "    def __init__(self, sample_dict) -> None:\n",
    "        super().__init__()\n",
    "        self.sample_dict = sample_dict\n",
    "        self.attribute_dict = {}\n",
    "        self.biosample_id = ''\n",
    "        self.content_dict = {}\n",
    "        self.is_title = False\n",
    "        self.is_paragraph = False\n",
    "        self.attribute_name = ''\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        if name == 'BioSample':\n",
    "            self.biosample_id = attrs['accession']\n",
    "        elif name == 'Title':\n",
    "            self.is_title = True\n",
    "        elif name == 'Paragraph':\n",
    "            self.is_paragraph = True\n",
    "        elif name == 'Attribute':\n",
    "            try:\n",
    "                self.attribute_name = attrs['harmonized_name']\n",
    "            except KeyError:\n",
    "                self.attribute_name = attrs['attribute_name']\n",
    "\n",
    "    def characters(self, content):\n",
    "        if self.is_title:\n",
    "            self.content_dict['title'] = content.lower()\n",
    "            self.is_title = False\n",
    "        elif self.is_paragraph:\n",
    "            self.content_dict['paragraph'] = content.lower()\n",
    "            self.is_paragraph = False\n",
    "        elif self.attribute_name != '':\n",
    "            self.attribute_dict[self.attribute_name] = content.lower()\n",
    "            self.attribute_name = ''\n",
    "        \n",
    "\n",
    "    def endElement(self, name):\n",
    "        if name == 'BioSample':\n",
    "            self.content_dict['attributes'] = self.attribute_dict\n",
    "            self.sample_dict[self.biosample_id] = self.content_dict\n",
    "            self.attribute_dict = {}\n",
    "            self.content_dict = {}\n",
    "    \n",
    "    def endDocument(self):\n",
    "        print('Finished parsing BioSamples XML file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished parsing BioSamples XML file\n",
      "Number of samples: 10000\n"
     ]
    }
   ],
   "source": [
    "sample_dict = {}\n",
    "biosamples_path = '../../data/biosamples/biosample_random_samples.xml'\n",
    "\n",
    "parser = xml.sax.make_parser()\n",
    "handler = BioSamplesMatcherHandler(sample_dict)\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "parser.parse(biosamples_path)\n",
    "print('Number of samples:', len(sample_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds a regex pattern to the default tokenizer to split on underscores\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "tokenizer = nlp.tokenizer\n",
    "\n",
    "infixes = nlp.Defaults.infixes + [r'[_~]']\n",
    "infix_re = spacy.util.compile_infix_regex(infixes)\n",
    "tokenizer.infix_finditer = infix_re.finditer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_dict = {}\n",
    "confident_attributes = {'tissue', 'cell_type', 'cell_line', 'cell_subtype', 'source_name'}\n",
    "\n",
    "# finds all returned matches from the matcher\n",
    "# - matcher looks at the title, paragraph, and attributes\n",
    "# - returns a dictionary of matches for each sample\n",
    "\n",
    "for biosample_id, content_dict in sample_dict.items():\n",
    "    cur_matches = {}\n",
    "    title = content_dict['title']\n",
    "    attributes = content_dict['attributes'] # dictionary of attributes\n",
    "\n",
    "    title_tokens = tokenizer(title)\n",
    "    attributes_tokens = {key: tokenizer(value) for key, value in attributes.items()}\n",
    "\n",
    "    title_matches = matcher(title_tokens, as_spans=True)\n",
    "    attribute_matches = {}\n",
    "    for key, value in attributes_tokens.items():\n",
    "        attribute_match = matcher(value, as_spans=True)\n",
    "        if len(attribute_match) > 0:\n",
    "            attribute_matches[key] = matcher(value, as_spans=True)\n",
    "\n",
    "\n",
    "    if len(title_matches) > 0:\n",
    "        cur_matches['title'] = title_matches\n",
    "    if len(attribute_matches) > 0:\n",
    "        cur_matches['attributes'] = attribute_matches\n",
    "\n",
    "    if 'paragraph' in content_dict:\n",
    "        paragraph = content_dict['paragraph']\n",
    "        paragraph_tokens = tokenizer(paragraph)\n",
    "        paragraph_matches = matcher(paragraph_tokens, as_spans=True)\n",
    "        \n",
    "        if len(paragraph_matches) > 0:\n",
    "            cur_matches['paragraph'] = paragraph_matches\n",
    "\n",
    "    if cur_matches == {}:\n",
    "        for key, value in attributes_tokens.items():\n",
    "            if key not in confident_attributes:\n",
    "                continue\n",
    "            attribute_match = attribute_matcher(value, as_spans=True)\n",
    "            if len(attribute_match) > 0:\n",
    "                cur_matches[key] = attribute_match\n",
    "\n",
    "        if len(attribute_matches) > 0:\n",
    "            cur_matches['attributes'] = attribute_matches\n",
    "            \n",
    "    matches_dict[biosample_id] = cur_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive results: 5309\n",
      "Number of negative results: 4691\n"
     ]
    }
   ],
   "source": [
    "positive_samples = {key: value for key, value in matches_dict.items() if len(value) > 0}\n",
    "negative_samples = {key: value for key, value in matches_dict.items() if len(value) == 0}\n",
    "print('Number of positive results:', len(positive_samples))\n",
    "print('Number of negative results:', len(negative_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing results to CSV file\n"
     ]
    }
   ],
   "source": [
    "# take the whole attribute stuff to check results\n",
    "with open('../../data/biosamples/results/biosample_tissue_locations_2.3.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['biosample_accession_id', 'biosample_url', 'title_match', 'paragraph_match', 'attribute_matches'])\n",
    "    for biosample_id, matches in matches_dict.items():\n",
    "        title_match = ''\n",
    "        paragraph_match = ''\n",
    "        attribute_matches = ''\n",
    "\n",
    "        for match_type, match in matches.items():\n",
    "            if match_type == 'title':\n",
    "                for token in match:\n",
    "                    title_match += token.text + ' '\n",
    "            elif match_type == 'paragraph':\n",
    "                for token in match:\n",
    "                    paragraph_match += token.text + ' '\n",
    "            elif match_type == 'attributes':\n",
    "                for attribute, match in match.items():\n",
    "                    attribute_matches += f'{attribute},'\n",
    "                    for token in match:\n",
    "                        attribute_matches += token.text + ' '\n",
    "        \n",
    "        url = f'https://www.ncbi.nlm.nih.gov/biosample/{biosample_id}'\n",
    "        writer.writerow([biosample_id, url, title_match, paragraph_match, attribute_matches])\n",
    "\n",
    "\n",
    "print('Finished writing results to CSV file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32669/4270646700.py:6: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  random_positive_samples = random.sample(positive_samples.items(), 50)\n",
      "/tmp/ipykernel_32669/4270646700.py:7: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  random_negative_samples = random.sample(negative_samples.items(), 50)\n"
     ]
    }
   ],
   "source": [
    "# creates a csv file of 50 random positive and negative samples for annotation\n",
    "# DO NOT RERUN\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "random_positive_samples = random.sample(positive_samples.items(), 50)\n",
    "random_negative_samples = random.sample(negative_samples.items(), 50)\n",
    "\n",
    "random_positive_samples = {key: value for key, value in random_positive_samples}\n",
    "random_negative_samples = {key: value for key, value in random_negative_samples}\n",
    " \n",
    "with open('../../data/biosamples/random_positive_samples.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['biosample_id', 'url', 'title', 'paragraph', 'attributes', 'eval'])\n",
    "    for biosample_id, matches in random_positive_samples.items():\n",
    "        title_match = ''\n",
    "        paragraph_match = ''\n",
    "        attribute_matches = ''\n",
    "\n",
    "        for match_type, match in matches.items():\n",
    "            if match_type == 'title':\n",
    "                for token in match:\n",
    "                    title_match += token.text + ' '\n",
    "            elif match_type == 'paragraph':\n",
    "                for token in match:\n",
    "                    paragraph_match += token.text + ' '\n",
    "            elif match_type == 'attributes':\n",
    "                for attribute, match in match.items():\n",
    "                    attribute_matches += f'{attribute},'\n",
    "                    for token in match:\n",
    "                        attribute_matches += token.text + ' '\n",
    "        url = f'https://www.ncbi.nlm.nih.gov/biosample/{biosample_id}'\n",
    "        writer.writerow([biosample_id, url, title_match, paragraph_match, attribute_matches])\n",
    "\n",
    "with open('../../data/biosamples/random_negative_samples.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['biosample_id', 'url', 'title', 'paragraph', 'attributes', 'eval'])\n",
    "    for biosample_id, matches in random_negative_samples.items():\n",
    "        if len(matches) > 0:\n",
    "            continue\n",
    "        title_match = ''\n",
    "        paragraph_match = ''\n",
    "        attribute_matches = ''\n",
    "\n",
    "        for match_type, match in matches.items():\n",
    "            if match_type == 'title':\n",
    "                for token in match:\n",
    "                    title_match += token.text + ' '\n",
    "            elif match_type == 'paragraph':\n",
    "                for token in match:\n",
    "                    paragraph_match += token.text + ' '\n",
    "            elif match_type == 'attributes':\n",
    "                for attribute, match in match.items():\n",
    "                    attribute_matches += f'{attribute},'\n",
    "                    for token in match:\n",
    "                        attribute_matches += token.text + ' '\n",
    "        url = f'https://www.ncbi.nlm.nih.gov/biosample/{biosample_id}'\n",
    "        writer.writerow([biosample_id, url, title_match, paragraph_match, attribute_matches])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 100 positive and 100 negative examples maually annotated to compare to found results\n",
    "\n",
    "### False positives\n",
    "- 'Null' being flagged as a term due to the weaker matcher being used when there are no matches\n",
    "- Whole term not being correctly captured due to partial term matches in BRENDA\n",
    "    - crude oil -> oil secretion\n",
    "- Weaker matches not being taken if there are existing matches in the biosample\n",
    "- Reversed word order of some terms in BRENDA not captured by the matcher\n",
    "    - 'epithelial buccal cells' != 'buccal epithelial cells'\n",
    "- Plurals not correctly captured\n",
    "- Some terms not included in BRENDA\n",
    "    - mediodorsal nucleus of the thalamus\n",
    "    - lateral habenula\n",
    "    - right femoral head\n",
    "- Center name and other attributes giving confounding results\n",
    "\n",
    "### False Negatives\n",
    "- Meaning not captured correctly for descriptions of sites\n",
    "- Some terms not included in BRENDA\n",
    "- Terms missed due to attirbute names not being listed as 'confident'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives: 72\n",
      "False positives: 28\n",
      "True negatives: 98\n",
      "False negatives: 2\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/biosamples/random_positive_samples.csv', 'r') as f:\n",
    "    df = pd.read_csv(f)\n",
    "    false_positives = df[df['eval'] == 'F'].shape[0]\n",
    "    true_positives = 100 - false_positives\n",
    "\n",
    "    print('True positives:', true_positives)\n",
    "    print('False positives:', false_positives)\n",
    "    # uncomment to view reasons for all false positives\n",
    "    # for row in df[df['eval'] == 'F'].iterrows():\n",
    "    #     print(row[1]['biosample_id'], ':', row[1]['notes'])\n",
    "\n",
    "with open('../../data/biosamples/random_negative_samples.csv', 'r') as f:\n",
    "    df = pd.read_csv(f)\n",
    "    false_negatives = df[df['eval'] == 'F'].shape[0]\n",
    "    true_negatives = 100 - false_negatives\n",
    "\n",
    "    print('True negatives:', true_negatives)\n",
    "    print('False negatives:', false_negatives)\n",
    "    # uncomment to view reasons for all false negatives\n",
    "    # for row in df[df['eval'] == 'F'].iterrows():\n",
    "    #     print(row[1]['biosample_id'], ':', row[1]['notes'])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
