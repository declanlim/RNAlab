{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2.1\n",
    "- Looks at the title and abstract (paragraph) for studies to find more relevant information\n",
    "- Added the additional synonym 'stool' for 'feces' (BTO_0000440)\n",
    "\n",
    "## Issues\n",
    "- More information may be available in the BioProject title and description\n",
    "- Can more patterns to the matcher that remove 'redundnant' start and end terms\n",
    "    - Either a direct search for the first and last terms \n",
    "    - Can add a custom extension attribute in spacy\n",
    "- Does not calculate the depth of specific terms in the BTO\n",
    "- Matcher will ignore preceding terms like not-___\n",
    "- Only looks at the first match in any values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready2 import *\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import xml.sax\n",
    "import csv\n",
    "import importlib\n",
    "import tissue_eval\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 6569\n"
     ]
    }
   ],
   "source": [
    "# Load the BTO ontology\n",
    "onto_path.append('../../data/ontologies/')\n",
    "onto = get_ontology('http://purl.obolibrary.org/obo/bto.owl').load()\n",
    "\n",
    "# classes dictionary: {class_name: class_label} \n",
    "#   - class_label is None if no label is found\n",
    "# class_synonyms dictionary: {class_name: [synonym1, synonym2, ...]}\n",
    "#   - synonym list is empty if no synonym is found\n",
    "classes = {c.name: c.label.first() for c in onto.classes()}\n",
    "class_synonyms = {c.name: c.hasExactSynonym + c.hasRelatedSynonym for c in onto.classes()}\n",
    "\n",
    "class_synonyms['BTO_0000440'] = class_synonyms['BTO_0000440'] + ['stool']\n",
    "\n",
    "# create a reverse mapping of classes and synonyms to BTO IDs\n",
    "classes_reverse = {c.label.first().lower(): c.name for c in onto.classes() if c.label != []}\n",
    "class_synonyms_reverse = {s.lower(): c for c, syn in class_synonyms.items() for s in syn}\n",
    "labels_reverse = {**classes_reverse, **class_synonyms_reverse}\n",
    "\n",
    "assert len(classes) == len(class_synonyms)\n",
    "print('Number of classes:', len(classes))\n",
    "\n",
    "# flatten the synonyms and class labels into a single set\n",
    "class_labels = {c for c in classes.values() if c is not None}\n",
    "class_synonyms_flattend = {s for syn in class_synonyms.values() for s in syn}\n",
    "bto_values = class_labels.union(class_synonyms_flattend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the depth of a term in an is_a hierarchy \n",
    "# - if there is more than one path to the root, the longest path is used\n",
    "bto_objects = {c.name: c for c in onto.classes()}\n",
    "\n",
    "def get_depth(bto_id):\n",
    "    depths = []\n",
    "    if bto_id == 'BTO_0000000':\n",
    "        return 0\n",
    "    \n",
    "    for bto_class in bto_objects[bto_id].is_a:\n",
    "        if bto_class == owl.Thing:\n",
    "            return 0\n",
    "        bto_class_type = type(bto_class)\n",
    "        if bto_class_type == ThingClass:\n",
    "            depths += [1 + get_depth(bto_class.name)]\n",
    "        elif bto_class_type == Restriction:\n",
    "            depths += [1 + get_depth(bto_class.value.name)]\n",
    "\n",
    "\n",
    "    return max(depths)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the depth for each class\n",
    "# depth_dict: {class_name: depth}\n",
    "depth_dict = {}\n",
    "\n",
    "get_depth('BTO_0000671')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a spacy matcher to match patterns of BRENDA terms and synonyms\n",
    "#  - patterns are created by tokenizing the BRENDA terms and synonyms\n",
    "#  - matcher looks at only direct matches\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "patterns = []\n",
    "\n",
    "for bto_value in bto_values:\n",
    "    pattern = [{'LOWER': token.lower_} for token in tokenizer(bto_value)]\n",
    "    patterns.append(pattern)\n",
    "    \n",
    "\n",
    "matcher.add('bto', patterns, greedy='LONGEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioSamplesMatcherHandler(xml.sax.ContentHandler):\n",
    "    def __init__(self, sample_dict) -> None:\n",
    "        super().__init__()\n",
    "        self.sample_dict = sample_dict\n",
    "        self.attribute_dict = {}\n",
    "        self.biosample_id = ''\n",
    "        self.content_dict = {}\n",
    "        self.is_title = False\n",
    "        self.is_paragraph = False\n",
    "        self.attribute_name = ''\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        if name == 'BioSample':\n",
    "            self.biosample_id = attrs['accession']\n",
    "        elif name == 'Title':\n",
    "            self.is_title = True\n",
    "        elif name == 'Paragraph':\n",
    "            self.is_paragraph = True\n",
    "        elif name == 'Attribute':\n",
    "            try:\n",
    "                self.attribute_name = attrs['harmonized_name']\n",
    "            except KeyError:\n",
    "                self.attribute_name = attrs['attribute_name']\n",
    "\n",
    "    def characters(self, content):\n",
    "        if self.is_title:\n",
    "            self.content_dict['title'] = content\n",
    "            self.is_title = False\n",
    "        elif self.is_paragraph:\n",
    "            self.content_dict['paragraph'] = content\n",
    "            self.is_paragraph = False\n",
    "        elif self.attribute_name != '':\n",
    "            self.attribute_dict[self.attribute_name] = content\n",
    "            self.attribute_name = ''\n",
    "        \n",
    "\n",
    "    def endElement(self, name):\n",
    "        if name == 'BioSample':\n",
    "            self.content_dict['attributes'] = self.attribute_dict\n",
    "            self.sample_dict[self.biosample_id] = self.content_dict\n",
    "            self.attribute_dict = {}\n",
    "            self.content_dict = {}\n",
    "    \n",
    "    def endDocument(self):\n",
    "        print('Finished parsing BioSamples XML file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished parsing BioSamples XML file\n",
      "Number of samples: 10000\n"
     ]
    }
   ],
   "source": [
    "sample_dict = {}\n",
    "attribute_dict = {}\n",
    "biosamples_path = '../../data/biosamples/biosample_random_samples.xml'\n",
    "\n",
    "parser = xml.sax.make_parser()\n",
    "handler = BioSamplesMatcherHandler(sample_dict)\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "parser.parse(biosamples_path)\n",
    "print('Number of samples:', len(sample_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds a regex pattern to the default tokenizer to split on underscores\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "tokenizer = nlp.tokenizer\n",
    "\n",
    "infixes = nlp.Defaults.infixes + [r'[_~]']\n",
    "infix_re = spacy.util.compile_infix_regex(infixes)\n",
    "tokenizer.infix_finditer = infix_re.finditer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_dict = {}\n",
    "\n",
    "# finds all returned matches from the matcher\n",
    "# - matcher looks at the title, paragraph, and attributes\n",
    "# - returns a dictionary of matches for each sample\n",
    "\n",
    "for biosample_id, content_dict in sample_dict.items():\n",
    "    cur_matches = {}\n",
    "    title = content_dict['title']\n",
    "    attributes = content_dict['attributes'] # dictionary of attributes\n",
    "\n",
    "    title_tokens = tokenizer(title)\n",
    "    attributes_tokens = {key: tokenizer(value) for key, value in attributes.items()}\n",
    "\n",
    "    title_matches = matcher(title_tokens, as_spans=True)\n",
    "    attribute_matches = {}\n",
    "    for key, value in attributes_tokens.items():\n",
    "        attribute_match = matcher(value, as_spans=True)\n",
    "        if len(attribute_match) > 0:\n",
    "            attribute_matches[key] = matcher(value, as_spans=True)[0]\n",
    "\n",
    "\n",
    "    if len(title_matches) > 0:\n",
    "        cur_matches['title'] = title_matches[0]\n",
    "    if len(attribute_matches) > 0:\n",
    "        cur_matches['attributes'] = attribute_matches\n",
    "\n",
    "    if 'paragraph' in content_dict:\n",
    "        paragraph = content_dict['paragraph']\n",
    "        paragraph_tokens = tokenizer(paragraph)\n",
    "        paragraph_matches = matcher(paragraph_tokens, as_spans=True)\n",
    "        \n",
    "        if len(paragraph_matches) > 0:\n",
    "            cur_matches['paragraph'] = paragraph_matches[0]\n",
    "\n",
    "    \n",
    "    matches_dict[biosample_id] = cur_matches    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing results to CSV file\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/biosamples/results/biosample_tissue_locations_2.1.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['biosample_accession_id', 'biosample_url', 'title_match', 'paragraph_match', 'attribute_matches'])\n",
    "    for biosample_id, matches in matches_dict.items():\n",
    "        title_match = ''\n",
    "        paragraph_match = ''\n",
    "        attribute_matches = ''\n",
    "\n",
    "        for match_type, match in matches.items():\n",
    "            if match_type == 'title':\n",
    "                title_match = match.text\n",
    "            elif match_type == 'paragraph':\n",
    "                paragraph_match = match.text\n",
    "            elif match_type == 'attributes':\n",
    "                for attribute, match in match.items():\n",
    "                    attribute_matches += f'{attribute}:{match.text},'\n",
    "        \n",
    "        biosample_url = f'https://www.ncbi.nlm.nih.gov/biosample/{biosample_id}'\n",
    "        writer.writerow([biosample_id, biosample_url, title_match, paragraph_match, attribute_matches])\n",
    "\n",
    "\n",
    "print('Finished writing results to CSV file')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
